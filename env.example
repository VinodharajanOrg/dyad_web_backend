# Disable TLS certificate validation for Splunk HEC (only for local dev)
NODE_TLS_REJECT_UNAUTHORIZED=0

# Server Configuration
PORT=3001
NODE_ENV=development

# Database - PostgreSQL
DATABASE_URL=postgresql://localhost:5432/dyad
# DATABASE_URL=postgresql://dyad_user:manisha@localhost:5432/dyad
DEFAULT_LIMIT=10
DEFAULT_LIMIT=10mb
ATTACHMENT_SIZE_LIMIT_MB=50mb
RATE_LIMIT_WINDOW_MS=60000
RATE_LIMIT_MAX=100

#AUTH_PROVIDER
AUTH_PROVIDER=keycloak
AUTH_ISSUER_URL=http://localhost:8080/realms/dyad-web
AUTH_CLIENT_ID=dyad-backend
AUTH_CLIENT_SECRET=NAeEkmz60N1vy1L1ZEm5OGF06MKbn6zh
AUTH_REDIRECT_URI=http://localhost:3001/api/auth/callback
AUTH_LOGOUT_ENDPOINT =/protocol/openid-connect/logout
# Endpoints
AUTH_TOKEN_ENDPOINT=/protocol/openid-connect/token
AUTH_USERINFO_ENDPOINT=/protocol/openid-connect/userinfo 

#frontend
FRONTEND_URL=http://localhost:3000

# Rate limiting
RATE_LIMIT_WINDOW_MS=900000
RATE_LIMIT_MAX=500
#constants
DEFAULT_LIMIT=10mb
ATTACHMENT_SIZE_LIMIT_MB=50mb

# Apps Directory - Where generated apps will be created
APPS_BASE_DIR=./apps

# ==========================================
# Containerization Configuration
# ==========================================

# Enable/disable containerization (true|false)
# When false, apps run as local processes instead of containers
CONTAINERIZATION_ENABLED=true

# Default package manager (pnpm|npm|yarn)
# Will be used if lock file auto-detection fails
DEFAULT_PACKAGE_MANAGER=pnpm

# Auto-kill processes/containers on occupied ports (true|false)
# When true, automatically stops any existing process or container using the target port
# When false, returns an error if port is already in use
AUTO_KILL_PORT=true

# Container inactivity timeout (in milliseconds)
# Containers will be automatically stopped after this period of inactivity
# Default: 600000 (10 minutes) || 300000 = 5 minutes reduced as its checking frequently and comparing the network usage
CONTAINER_INACTIVITY_TIMEOUT=120000

# Container CPU limit (e.g., 0.5, 1, 2)
# Limits the number of CPUs a container can use
# Default: 1
CONTAINER_CPU_LIMIT=4

# Container memory limit (e.g., 512m, 1g, 2g)
# Limits the amount of memory a container can use
# Default: 1g
CONTAINER_MEMORY_LIMIT=1g

# ==========================================
# Logging Configuration
# ==========================================

# Log level (debug|info|warn|error)
LOG_LEVEL=info

# Log format (console|json)
# console: Human-readable colored output
# json: Structured JSON output for log aggregation systems
LOG_FORMAT=console

# Optional: External observability platform integration
# LOG_HTTP_ENDPOINT=https://your-observability-platform.com/api/logs
# LOG_HTTP_AUTH=Bearer your-auth-token

# Example Splunk HEC (HTTP Event Collector):
# LOG_HTTP_ENDPOINT=https://http-inputs-splunk.example.com:8088/services/collector/event
# LOG_HTTP_AUTH=Splunk your-hec-token

# Example Grafana Loki:
# LOG_HTTP_ENDPOINT=https://loki.example.com/loki/api/v1/push
# LOG_HTTP_AUTH=Basic base64-encoded-credentials

# Container engine to use (docker|podman|tanzu|kubernetes)
# This determines which container engine handler is loaded
CONTAINERIZATION_ENGINE=podman

# Docker-specific configuration
DOCKER_SOCKET=/var/run/docker.sock
# Using custom optimized image with pre-cached dependencies for 90% faster startup
# Build with: ./scripts/build-optimized-image.sh (Unix) or scripts\build-optimized-image.bat (Windows)
DOCKER_IMAGE=node:22-alpine
DOCKER_DEFAULT_PORT=32100

# Podman-specific configuration
PODMAN_SOCKET=/var/folders/jh/ktqywsjd0t35b97znh6088qm0000gp/T/podman/podman-machine-default.sock
# Using custom optimized image with pre-cached dependencies for 90% faster startup
# Build with: ./scripts/build-optimized-image.sh (Unix) or scripts\build-optimized-image.bat (Windows)
PODMAN_IMAGE=node:22-alpine
PODMAN_DEFAULT_PORT=32100

# VMware Tanzu configuration (for future use)
# TANZU_API_URL=https://tanzu-api.example.com
# TANZU_NAMESPACE=development
# TANZU_IMAGE=node:22-alpine

# Kubernetes configuration (for future use)
# Uncomment to use Kubernetes
# CONTAINERIZATION_ENGINE=kubernetes
# KUBECONFIG=~/.kube/config
# K8S_NAMESPACE=default
# K8S_IMAGE=node:22-alpine

# ==========================================
# AI API Keys (Fallback if not in database)
# ==========================================
# These are used only if settings are not available in database
# For production, use /api/settings to manage API keys

# Default AI Model Configuration
# Format: provider:model-id (e.g., openai:gpt-4o, anthropic:claude-3-5-sonnet-20241022)
# âœ… GPT-5.1 is now available! Using latest model.
# Available GPT-5.1 variants: gpt-5.1, gpt-5.1-codex-max, gpt-5.1-codex, gpt-5.1-2025-11-13
# Other models: gpt-4o, gpt-4-turbo, gpt-4, gpt-3.5-turbo
# Gemini models: gemini-2.5-pro, gemini-2.5-flash, gemini-2.0-flash-exp
DEFAULT_AI_MODEL=google:gemini-2.5-flash

# Fallback AI Model (used if selected model API key is not configured)
# This ensures the system always has a working model available
FALLBACK_AI_MODEL=openai:gpt-4o

# Default Chat Mode (auto-code|ask|agent|custom)
# auto-code: Automatically write code and files
# ask: Conversational mode without code generation
# agent: Autonomous agent mode with multi-step reasoning
DEFAULT_CHAT_MODE=auto-code

# OpenAI API Key (RECOMMENDED - More reliable than free Gemini)
OPENAI_API_KEY=your-openai-api-key-here

# Anthropic API Key (Alternative - Claude models)
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here

# Google AI API Key (Gemini 2.5 Pro configured)
# Models available: gemini-2.5-pro, gemini-2.5-flash, gemini-2.0-flash-exp
# Note: There is no Gemini 3 yet - latest is Gemini 2.5
GOOGLE_GENERATIVE_AI_API_KEY=your-google-api-key-here

# Azure Foundry Configuration (Anthropic via Azure)
# Example endpoint: https://rajib-mi8u3j4o-eastus2.services.ai.azure.com/models
# Example deployment: claude-sonnet-4-5
AZURE_FOUNDRY_API_KEY=your-azure-foundry-api-key-here

# Custom API Endpoint (for Azure Foundry or custom proxies)
# Use this to override the default endpoint for any provider
# Note: Use /models endpoint, not /anthropic/ - the provider handles the model path
CUSTOM_API_ENDPOINT=https://your-endpoint.services.ai.azure.com/models

# Azure OpenAI Configuration
# AZURE_OPENAI_API_KEY=...
# AZURE_OPENAI_ENDPOINT=https://....openai.azure.com/
# AZURE_OPENAI_DEPLOYMENT=...
# K8S_IMAGE=node:22-alpine